{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Ç´„Éº„Éç„É´„É™„Çπ„Çø„Éº„Éà„ÅÆÊôÇ„ÅØ„Åì„ÅÆ„Çª„É´„ÇíÂÆüË°å„Åó„Å™„Åè„Å¶„ÇÇOK\n",
    "!wget https://bootstrap.pypa.io/get-pip.py\n",
    "!python get-pip.py\n",
    "%pip install tokenizers fugashi ipadic accelerate==0.20.3 seaborn\n",
    "%pip install transformers datasets scikit-learn\n",
    "!wget https://github.com/ids-cv/wrime/raw/master/wrime-ver1.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrime = pd.read_table('wrime-ver1.tsv')\n",
    "emotion_names = ['Joy', 'Sadness', 'Anticipation', 'Surprise', 'Anger', 'Fear', 'Disgust', 'Trust']\n",
    "emotion_names_jp = ['Âñú„Å≥', 'ÊÇ≤„Åó„Åø', 'ÊúüÂæÖ', 'È©ö„Åç', 'ÊÄí„Çä', 'ÊÅê„Çå', 'Â´åÊÇ™', '‰ø°È†º']\n",
    "num_labels = len(emotion_names)\n",
    "\n",
    "df_wrime['readers_emotion_intensities'] = df_wrime.apply(lambda x: [x['Avg. Readers_' + name] for name in emotion_names], axis=1)\n",
    "\n",
    "# removing samples with less emotion intensities\n",
    "# (max.readers_emotion_intensities must be 2 or more)\n",
    "is_target = df_wrime['readers_emotion_intensities'].map(lambda x: max(x) >= 2)\n",
    "df_wrime_target = df_wrime[is_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 10942\n",
      "valid: 3647\n",
      "test: 3648\n"
     ]
    }
   ],
   "source": [
    "# Divide into train, validation, and test sets\n",
    "train_data, test_valid_data = train_test_split(df_wrime_target, test_size=0.4, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_valid_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print('train:', len(train_data))\n",
    "print('valid:', len(valid_data))\n",
    "print('test:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰ΩøÁî®„Åô„Çã„É¢„Éá„É´„ÇíÊåáÂÆö„Åó„Å¶„ÄÅTokenizer„ÇíË™≠„ÅøËæº„ÇÄ\n",
    "checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7ff2b69a55a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89745cdc81e479b8606ca89aa26a419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10942 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8603214ef0b64a58a98b3b318cab78df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf9894091054cde9f488a6ec1986e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # ÂâçÂá¶ÁêÜÈñ¢Êï∞: tokenize_function\n",
    "# # ÊÑüÊÉÖÂº∑Â∫¶„ÅÆÊ≠£Ë¶èÂåñÔºàÁ∑èÂíå=1Ôºâ„ÇÇÂêåÊôÇ„Å´ÂÆüÊñΩ„Åô„Çã\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['Sentence'], truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    tokenized_batch['labels'] = [x / np.sum(x) for x in batch['readers_emotion_intensities']]\n",
    "    return tokenized_batch\n",
    "\n",
    "# TransformersÁî®„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂΩ¢Âºè„Å´Â§âÊèõ\n",
    "# pandas.DataFrame -> datasets.Dataset\n",
    "target_columns = ['Sentence', 'readers_emotion_intensities']\n",
    "train_dataset = Dataset.from_pandas(train_data[target_columns])\n",
    "valid_dataset = Dataset.from_pandas(valid_data[target_columns])\n",
    "test_dataset = Dataset.from_pandas(test_data[target_columns])\n",
    "\n",
    "# ÂâçÂá¶ÁêÜÔºàtokenize_functionÔºâ „ÇíÈÅ©Áî®\n",
    "train_tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_tokenized_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1159/936320056.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/training\n",
    "metric = load_metric(\"accuracy\")\n",
    "# categorical_accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    label_ids = np.argmax(labels, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install accelerate transformers[torch] -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1368' max='1368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1368/1368 04:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288242</td>\n",
       "      <td>0.534138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270140</td>\n",
       "      <td>0.614478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.296600</td>\n",
       "      <td>0.260736</td>\n",
       "      <td>0.618042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.296600</td>\n",
       "      <td>0.252921</td>\n",
       "      <td>0.646285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.258900</td>\n",
       "      <td>0.243772</td>\n",
       "      <td>0.673704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.258900</td>\n",
       "      <td>0.241411</td>\n",
       "      <td>0.668220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1368, training_loss=0.26929717036018597, metrics={'train_runtime': 261.7104, 'train_samples_per_second': 41.81, 'train_steps_per_second': 5.227, 'total_flos': 2879116261933056.0, 'train_loss': 0.26929717036018597, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformers „ÅÆ Trainer „ÇíÁî®„ÅÑ„Çã\n",
    "# https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "# Ë®ìÁ∑¥ÊôÇ„Å´error ‚Üí ‰∏ä„ÅÆ„Ç≥„Éº„Éâ„Çª„É´„Çí„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„Éà ‚Üí ÂÆüË°å ‚Üí (‰ªÆÊÉ≥Áí∞Â¢É„Çídeactivate + restart vscode)„ÇÇ„Åó„Åè„ÅØ(„Ç´„Éº„Éç„É´„É™„Çπ„Çø„Éº„Éà) ‚Üí ‰∏ä„ÅÆ„Ç≥„Éº„Éâ„Çª„É´„Çí„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„Éà ‚Üí run all the cells again„ÅßËß£Ê±∫\n",
    "\n",
    "# Ë®ìÁ∑¥ÊôÇ„ÅÆË®≠ÂÆö„Çí‰øÆÊ≠£\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=8, # originally 8\n",
    "    num_train_epochs=1.0, # originally 1\n",
    "    evaluation_strategy=\"steps\", eval_steps=200)  # 200„Çπ„ÉÜ„ÉÉ„ÉóÊØé„Å´Ê§úË®º„Éá„Éº„Çø„ÅßË©ï‰æ°„Åô„Çã\n",
    "\n",
    "# Trainer„ÇíÁîüÊàê\n",
    "newtrainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=valid_tokenized_dataset,  # Ê§úË®º„Éá„Éº„Çø„Çí‰ΩøÁî®„Åô„Çã\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Ë®ìÁ∑¥„ÇíÂÆüË°å\n",
    "newtrainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## „ÉÜ„Çπ„Éà„Éá„Éº„Çø„Å´„É¢„Éá„É´„Çí„Ç¢„Éó„É©„Ç§„ÄÅÊ∑∑ÂêàË°åÂàó„Çí‰ΩúÊàê„ÉªF1„Çπ„Ç≥„Ç¢„ÇíÂêÑÊÑüÊÉÖ„É©„Éô„É´„Å´„Å§„ÅÑ„Å¶Ë®àÁÆó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.delftstack.com/ja/howto/numpy/numpy-softmax/\n",
    "def np_softmax(x):\n",
    "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÉÜ„Ç≠„Çπ„Éà„ÇíÊÑüÊÉÖËß£Êûê„Åô„ÇãÈñ¢Êï∞\n",
    "def analyze_emotion(text):\n",
    "    # Êé®Ë´ñ„É¢„Éº„Éâ\n",
    "    model.eval()\n",
    "\n",
    "    # ÂÖ•Âäõ„Éá„Éº„ÇøÂ§âÊèõ + Êé®Ë´ñ\n",
    "    tokens = tokenizer(text, truncation=True, return_tensors=\"pt\")\n",
    "    tokens.to(model.device)\n",
    "    preds = model(**tokens)\n",
    "    prob = np_softmax(preds.logits.cpu().detach().numpy()[0])\n",
    "    out_dict = {n: p for n, p in zip(emotion_names_jp, prob)}\n",
    "    out_list = list(out_dict.values())\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÁµêÊûú„Çí‰øùÂ≠ò„Åô„ÇãÁ©∫„ÅÆ„É™„Çπ„Éà„Çí‰ΩúÊàê\n",
    "predicted_labels = []\n",
    "\n",
    "# test_tokenized_dataset„Åã„ÇâSentence„Ç´„É©„É†„ÅÆ„Éá„Éº„Çø„ÇíÂèñÂæó\n",
    "sentences = test_tokenized_dataset['Sentence']\n",
    "\n",
    "# ÂêÑ„ÉÜ„Ç≠„Çπ„Éà„Å´analyze_emotionÈñ¢Êï∞„ÇíÈÅ©Áî®„Åó„ÄÅÁµêÊûú„Çí„É™„Çπ„Éà„Å´‰øùÂ≠ò\n",
    "for text in sentences:\n",
    "    result = analyze_emotion(text)\n",
    "    predicted_labels.append(result)\n",
    "\n",
    "true_labels = test_tokenized_dataset['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰∫àÊ∏¨ÁµêÊûú„Å®Áúü„ÅÆ„É©„Éô„É´„ÇíDataFrame„Å´Â§âÊèõ\n",
    "predicted_df = pd.DataFrame(predicted_labels, columns=emotion_names_jp)\n",
    "true_df = pd.DataFrame(true_labels, columns=emotion_names_jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame„ÅÆÂêÑË°å„ÇíÊõ¥Êñ∞„Åó„Å¶„ÄÅÊúÄÂ§ßÂÄ§„Å´1„ÄÅ„Åù„Çå‰ª•Â§ñ„Å´0„ÇíÊåÅ„Å§„Çà„ÅÜ„Å´„Åô„Çã\n",
    "def update_dataframe(df):\n",
    "    for index, row in df.iterrows():\n",
    "        max_value = row.max()\n",
    "        df.loc[index] = (row == max_value).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_process_values = update_dataframe(predicted_df)\n",
    "true_process_values = update_dataframe(true_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂêÑDataFrame„Åã„ÇâÊúÄÂ§ß„ÅÆÊÑüÊÉÖ„ÇíÊäΩÂá∫\n",
    "def get_max_emotions(df):\n",
    "    max_emotions = []\n",
    "    for index, row in df.iterrows():\n",
    "        max_emotions.append(row.index[row == 1].tolist())\n",
    "    return pd.DataFrame({'Emotions': max_emotions})\n",
    "\n",
    "predicted_emotions = get_max_emotions(predicted_df)\n",
    "true_emotions = get_max_emotions(true_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_emotions„ÅÆ„É™„Çπ„Éà„Çµ„Ç§„Ç∫„Åå2‰ª•‰∏ä„ÅÆÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ2„Å§„ÇíÂÆüË°å\n",
    "# 1:pred„É™„Çπ„Éà„ÅÆÊÑüÊÉÖ„Ååtrue„É™„Çπ„Éà„Å´„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅpred„É™„Çπ„Éà„Å®‰∏ÄËá¥„Åô„ÇãÊÑüÊÉÖ„ÇíÈô§„ÅÑ„Å¶true„É™„Çπ„ÉàÂÜÖ„ÅÆÊÑüÊÉÖ„ÇíÂâäÈô§ \n",
    "# 2:pred„É™„Çπ„Éà„ÅÆÊÑüÊÉÖ„Ååtrue„É™„Çπ„Éà„Å´„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅ‰∏°„É™„Çπ„Éà„ÅÆÊÑüÊÉÖ„Çí„Åô„Åπ„Å¶ÂâäÈô§„Åó„Å¶Á©∫„É™„Çπ„Éà„Å´„Åô„Çã‚ÜíÊ∑∑ÂêåË°åÂàó„ÉªF1„Çπ„Ç≥„Ç¢Ë®àÁÆó„Å´„ÅØÂê´„Åæ„Å™„ÅÑ„Ç®„É≥„Éà„É™„Å®„Åó„Å¶Êâ±„ÅÜ\n",
    "def remove_extra_emotions(predicted_emotions, true_emotions):\n",
    "    for idx, (pred, true) in zip(predicted_emotions.index, zip(predicted_emotions['Emotions'], true_emotions['Emotions'])):\n",
    "        if len(true) >= 2:\n",
    "            true_emotions.at[idx, 'Emotions'] = [emotion for emotion in true if emotion in pred] if any(emotion in pred for emotion in true) else []\n",
    "\n",
    "remove_extra_emotions(predicted_emotions, true_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê∑∑ÂêåË°åÂàó„ÅÆ‰ΩúÊàê\n",
    "confusion_matrix_data = pd.DataFrame(0, index=emotion_labels, columns=emotion_labels)\n",
    "for pred, true in zip(predicted_emotions['Emotions'], true_emotions['Emotions']):\n",
    "    for pred_label in pred:\n",
    "        if pred_label in emotion_labels:\n",
    "            for true_label in true:\n",
    "                if true_label in emotion_labels:\n",
    "                    confusion_matrix_data.at[true_label, pred_label] += 1\n",
    "\n",
    "# ÂêÑÂàó„Å®ÂêÑË°å„Å´ÂêàË®àÂÄ§„ÇíËøΩÂä†\n",
    "confusion_matrix_data['ÂêàË®à'] = confusion_matrix_data.sum(axis=1)\n",
    "confusion_matrix_data.loc['ÂêàË®à'] = confusion_matrix_data.sum()\n",
    "\n",
    "# Ê∑∑ÂêåË°åÂàó„Å´ÊòéË®ò\n",
    "confusion_matrix_data.index.name = '‰∫àÊ∏¨ÂÄ§'\n",
    "confusion_matrix_data.columns.name = 'Ê≠£Ëß£ÂÄ§'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ê≠£Ëß£ÂÄ§</th>\n",
       "      <th>Âñú„Å≥</th>\n",
       "      <th>ÊÇ≤„Åó„Åø</th>\n",
       "      <th>ÊúüÂæÖ</th>\n",
       "      <th>È©ö„Åç</th>\n",
       "      <th>ÊÄí„Çä</th>\n",
       "      <th>ÊÅê„Çå</th>\n",
       "      <th>Â´åÊÇ™</th>\n",
       "      <th>‰ø°È†º</th>\n",
       "      <th>ÂêàË®à</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‰∫àÊ∏¨ÂÄ§</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Âñú„Å≥</th>\n",
       "      <td>782</td>\n",
       "      <td>43</td>\n",
       "      <td>65</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ÊÇ≤„Åó„Åø</th>\n",
       "      <td>22</td>\n",
       "      <td>496</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ÊúüÂæÖ</th>\n",
       "      <td>73</td>\n",
       "      <td>40</td>\n",
       "      <td>724</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>È©ö„Åç</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>31</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ÊÄí„Çä</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ÊÅê„Çå</th>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Â´åÊÇ™</th>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‰ø°È†º</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ÂêàË®à</th>\n",
       "      <td>944</td>\n",
       "      <td>759</td>\n",
       "      <td>898</td>\n",
       "      <td>470</td>\n",
       "      <td>0</td>\n",
       "      <td>231</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ê≠£Ëß£ÂÄ§   Âñú„Å≥  ÊÇ≤„Åó„Åø   ÊúüÂæÖ   È©ö„Åç  ÊÄí„Çä   ÊÅê„Çå   Â´åÊÇ™  ‰ø°È†º    ÂêàË®à\n",
       "‰∫àÊ∏¨ÂÄ§                                            \n",
       "Âñú„Å≥   782   43   65   56   0    8    5   0   959\n",
       "ÊÇ≤„Åó„Åø   22  496   32   27   0   24   32   0   633\n",
       "ÊúüÂæÖ    73   40  724   36   0   10   20   0   903\n",
       "È©ö„Åç    42   42   31  298   0   11   17   0   441\n",
       "ÊÄí„Çä     1    9    2    4   0    2   26   0    44\n",
       "ÊÅê„Çå    10   84   29   31   0  166   25   0   345\n",
       "Â´åÊÇ™     9   45   13   15   0   10  152   0   244\n",
       "‰ø°È†º     5    0    2    3   0    0    0   0    10\n",
       "ÂêàË®à   944  759  898  470   0  231  277   0  3579"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂêÑÊÑüÊÉÖ„É©„Éô„É´„ÅÆPrecision„ÄÅRecall„ÄÅF1„Çπ„Ç≥„Ç¢:\n",
      "     Precision    Recall  F1 Score\n",
      "Âñú„Å≥    0.815433  0.828390  0.821860\n",
      "ÊÇ≤„Åó„Åø   0.783570  0.653491  0.712644\n",
      "ÊúüÂæÖ    0.801772  0.806236  0.803998\n",
      "È©ö„Åç    0.675737  0.634043  0.654226\n",
      "ÊÄí„Çä    0.000000  0.000000  0.000000\n",
      "ÊÅê„Çå    0.481159  0.718615  0.576389\n",
      "Â´åÊÇ™    0.622951  0.548736  0.583493\n",
      "‰ø°È†º    0.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# ÂêÑÊÑüÊÉÖ„É©„Éô„É´„ÅÆPrecision„ÄÅRecall„ÄÅF1„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó\n",
    "f1_scores = {}\n",
    "for emotion_label in emotion_names_jp:\n",
    "    tp = confusion_matrix_data.at[emotion_label, emotion_label]\n",
    "    fp = confusion_matrix_data.loc[emotion_label, 'ÂêàË®à'] - tp\n",
    "    fn = confusion_matrix_data.loc['ÂêàË®à', emotion_label] - tp\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    f1_scores[emotion_label] = {'Precision': precision, 'Recall': recall, 'F1 Score': f1_score}\n",
    "\n",
    "# ÁµêÊûú„ÇíË°®Á§∫\n",
    "f1_scores_df = pd.DataFrame.from_dict(f1_scores, orient='index')\n",
    "print(\"ÂêÑÊÑüÊÉÖ„É©„Éô„É´„ÅÆPrecision„ÄÅRecall„ÄÅF1„Çπ„Ç≥„Ç¢:\")\n",
    "print(f1_scores_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
