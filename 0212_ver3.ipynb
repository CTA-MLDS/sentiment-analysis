{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # カーネルリスタートの時はこのセルを実行しなくてもOK\n",
    "# !wget https://bootstrap.pypa.io/get-pip.py\n",
    "# !python get-pip.py\n",
    "# %pip install tokenizers fugashi ipadic accelerate==0.20.3 seaborn\n",
    "# %pip install transformers datasets scikit-learn\n",
    "# !wget https://github.com/ids-cv/wrime/raw/master/wrime-ver1.tsv\n",
    "# %pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrime = pd.read_table('wrime-ver1.tsv')\n",
    "emotion_names = ['Joy', 'Sadness', 'Anticipation', 'Surprise', 'Anger', 'Fear', 'Disgust', 'Trust']\n",
    "emotion_names_jp = ['喜び', '悲しみ', '期待', '驚き', '怒り', '恐れ', '嫌悪', '信頼']\n",
    "num_labels = len(emotion_names)\n",
    "\n",
    "df_wrime['readers_emotion_intensities'] = df_wrime.apply(lambda x: [x['Avg. Readers_' + name] for name in emotion_names], axis=1)\n",
    "\n",
    "# removing samples with less emotion intensities\n",
    "# (max.readers_emotion_intensities must be 2 or more)\n",
    "is_target = df_wrime['readers_emotion_intensities'].map(lambda x: max(x) >= 2)\n",
    "df_wrime_target = df_wrime[is_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['Sentence', 'readers_emotion_intensities']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataframe(df):\n",
    "    if 'readers_emotion_intensities' not in df.columns:\n",
    "        raise ValueError(\"DataFrame does not contain 'readers_emotion_intensities' column.\")\n",
    "    \n",
    "    # DataFrameをコピーして新しいDataFrameを作成\n",
    "    updated_df = df.copy()\n",
    "    \n",
    "    # 'readers_emotion_intensities'列の各要素を更新\n",
    "    for index, row in updated_df.iterrows():\n",
    "        max_value = max(row['readers_emotion_intensities'])\n",
    "        updated_df.at[index, 'readers_emotion_intensities'] = [int(value == max_value) for value in row['readers_emotion_intensities']]\n",
    "    \n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrime_target_updated = update_dataframe(df_wrime_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readers_emotion_intensities 列に1が2回以上登場する行を削除する\n",
    "df_wrime_target_updated_filtered = df_wrime_target_updated[df_wrime_target_updated['readers_emotion_intensities'].apply(lambda x: x.count(1) < 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1435/2427799980.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_wrime_target_updated_filtered['emotion'] = emotion_labels\n"
     ]
    }
   ],
   "source": [
    "# 感情ラベルを保存するための空のリストを作成\n",
    "emotion_labels = []\n",
    "\n",
    "# 各行に対して処理を行う\n",
    "for index, row in df_wrime_target_updated_filtered.iterrows():\n",
    "    # 1が格納されているインデックスを取得し、対応する感情ラベルを取得する\n",
    "    emotions = [emotion_names_jp[i] for i, val in enumerate(row['readers_emotion_intensities']) if val == 1]\n",
    "    # 感情ラベルがない場合は空文字列を追加する\n",
    "    if len(emotions) == 0:\n",
    "        emotion_labels.append('')\n",
    "    else:\n",
    "        # 複数の感情がある場合はカンマ区切りの文字列に変換して追加する\n",
    "        emotion_labels.append(', '.join(emotions))\n",
    "\n",
    "# 新しい感情カラムをデータフレームに追加する\n",
    "df_wrime_target_updated_filtered['emotion'] = emotion_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "喜び     4441\n",
      "期待     4053\n",
      "悲しみ    2900\n",
      "驚き     1955\n",
      "恐れ     1591\n",
      "嫌悪     1024\n",
      "怒り      197\n",
      "信頼       45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_wrime_target_updated_filtered['emotion'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "悲しみ    4441\n",
      "驚き     4441\n",
      "喜び     4441\n",
      "期待     4441\n",
      "恐れ     4441\n",
      "信頼     4441\n",
      "怒り     4441\n",
      "嫌悪     4441\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 各感情ラベルごとのサンプル数をカウント\n",
    "emotion_counts = df_wrime_target_updated_filtered['emotion'].value_counts()\n",
    "\n",
    "# オーバーサンプリングのためのサンプル数の計算\n",
    "max_samples = emotion_counts.max()  # 最大のサンプル数を基準にする\n",
    "min_samples = emotion_counts.min()\n",
    "\n",
    "# オーバーサンプリング\n",
    "ros = RandomOverSampler(sampling_strategy={label: max_samples for label in emotion_counts.index})\n",
    "oversampled_X, oversampled_y = ros.fit_resample(df_wrime_target_updated_filtered['Sentence'].values.reshape(-1, 1), df_wrime_target_updated_filtered['emotion'])\n",
    "\n",
    "# データフレームに変換\n",
    "df_wrime_target_updated_filtered_oversampled = pd.DataFrame({'Sentence': oversampled_X.flatten(), 'emotion': oversampled_y})\n",
    "\n",
    "\n",
    "# 結果の表示\n",
    "print(df_wrime_target_updated_filtered_oversampled['emotion'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emotion_to_binary(emotion_label):\n",
    "    # ラベルが存在する場合は対応するバイナリデータを作成\n",
    "    if emotion_label in emotion_names_jp:\n",
    "        # インデックスを取得\n",
    "        idx = emotion_names_jp.index(emotion_label)\n",
    "        # インデックスに対応する位置を1、それ以外を0とするリストを作成\n",
    "        binary_data = [1 if i == idx else 0 for i in range(len(emotion_names_jp))]\n",
    "        return binary_data\n",
    "    else:\n",
    "        # ラベルが存在しない場合はエラーメッセージを出力してNoneを返す\n",
    "        print(\"Invalid emotion label!\")\n",
    "        return [0] * len(emotion_names_jp)\n",
    "\n",
    "# NaNを埋める\n",
    "df_wrime_target_updated_filtered_oversampled['readers_emotion_intensities'] = df_wrime_target_updated_filtered_oversampled['emotion'].apply(convert_emotion_to_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用するモデルを指定して、Tokenizerを読み込む\n",
    "checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into train, validation, and test sets\n",
    "train_data, test_valid_data = train_test_split(df_wrime_target_updated_filtered_oversampled, test_size=0.4, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_valid_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['Sentence'], truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    tokenized_batch['labels'] = [x / np.sum(x) for x in batch['readers_emotion_intensities']]\n",
    "    return tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7efb0066ef80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8dd529cb1b4500962bd174bb75977d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c8e18f4eb24587a3f0976b418f54f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7106 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aae951ca604995a9e30292dbdba7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7106 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transformers用のデータセット形式に変換\n",
    "# pandas.DataFrame -> datasets.Dataset\n",
    "target_columns = ['Sentence', 'readers_emotion_intensities']\n",
    "train_dataset = Dataset.from_pandas(train_data[target_columns])\n",
    "valid_dataset = Dataset.from_pandas(valid_data[target_columns])\n",
    "test_dataset = Dataset.from_pandas(test_data[target_columns])\n",
    "\n",
    "# 前処理（tokenize_function） を適用\n",
    "train_tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_tokenized_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1435/2983350284.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/training\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    label_ids = np.argmax(labels, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install accelerate transformers[torch] -U\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練時にerror → 上のコードセルをコメントアウト → 実行 → (仮想環境をdeactivate + restart vscode)もしくは(カーネルリスタート) → 上のコードセルをコメントアウト → run all the cells againで解決\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3999' max='3999' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3999/3999 17:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.244400</td>\n",
       "      <td>0.163124</td>\n",
       "      <td>0.746834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.125771</td>\n",
       "      <td>0.808331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.109874</td>\n",
       "      <td>0.842387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.102253</td>\n",
       "      <td>0.853786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.090913</td>\n",
       "      <td>0.876724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.097098</td>\n",
       "      <td>0.878694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.095092</td>\n",
       "      <td>0.885167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test_trainer/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory test_trainer/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory test_trainer/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory test_trainer/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory test_trainer/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory test_trainer/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory test_trainer/checkpoint-3500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3999, training_loss=0.08923298539325994, metrics={'train_runtime': 1042.4949, 'train_samples_per_second': 61.341, 'train_steps_per_second': 3.836, 'total_flos': 1.6826332180414464e+16, 'train_loss': 0.08923298539325994, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformers の Trainer を用いる\n",
    "# https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "# 訓練時の設定を修正\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=16, # originally 8\n",
    "    num_train_epochs=3.0, # originally 1\n",
    "    evaluation_strategy=\"steps\", eval_steps=500)  # 500ステップ毎に検証データで評価する\n",
    "\n",
    "# Trainerを生成\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=valid_tokenized_dataset,  # 検証データを使用する\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 訓練を実行\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.delftstack.com/ja/howto/numpy/numpy-softmax/\n",
    "def np_softmax(x):\n",
    "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストを感情解析する関数\n",
    "def analyze_emotion(text):\n",
    "    # 推論モード\n",
    "    model.eval()\n",
    "\n",
    "    # 入力データ変換 + 推論\n",
    "    tokens = tokenizer(text, truncation=True, return_tensors=\"pt\")\n",
    "    tokens.to(model.device)\n",
    "    preds = model(**tokens)\n",
    "    prob = np_softmax(preds.logits.cpu().detach().numpy()[0])\n",
    "    out_dict = {n: p for n, p in zip(emotion_names_jp, prob)}\n",
    "    out_list = list(out_dict.values())\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を保存する空のリストを作成\n",
    "predicted_labels = []\n",
    "\n",
    "# test_tokenized_datasetからSentenceカラムのデータを取得\n",
    "sentences = test_tokenized_dataset['Sentence']\n",
    "\n",
    "# 各テキストにanalyze_emotion関数を適用し、結果をリストに保存\n",
    "for text in sentences:\n",
    "    result = analyze_emotion(text)\n",
    "    predicted_labels.append(result)\n",
    "\n",
    "true_labels = test_tokenized_dataset['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果と真のラベルをDataFrameに変換\n",
    "predicted_df = pd.DataFrame(predicted_labels, columns=emotion_names_jp)\n",
    "true_df = pd.DataFrame(true_labels, columns=emotion_names_jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameの各行を更新して、最大値に1、それ以外に0を持つようにする\n",
    "def update_dataframe(df):\n",
    "    for index, row in df.iterrows():\n",
    "        max_value = row.max()\n",
    "        df.loc[index] = (row == max_value).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_process_values = update_dataframe(predicted_df)\n",
    "true_process_values = update_dataframe(true_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各DataFrameから最大の感情を抽出\n",
    "def get_max_emotions(df):\n",
    "    max_emotions = []\n",
    "    for index, row in df.iterrows():\n",
    "        max_emotions.append(row.index[row == 1].tolist())\n",
    "    return pd.DataFrame({'Emotions': max_emotions})\n",
    "\n",
    "predicted_emotions = get_max_emotions(predicted_df)\n",
    "true_emotions = get_max_emotions(true_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_emotionsのリストサイズが2以上の場合、以下の2つを実行\n",
    "# 1:predリストの感情がtrueリストにある場合は、predリストと一致する感情を除いてtrueリスト内の感情を削除 \n",
    "# 2:predリストの感情がtrueリストにない場合は、両リストの感情をすべて削除して空リストにする→混同行列・F1スコア計算には含まないエントリとして扱う\n",
    "def remove_extra_emotions(predicted_emotions, true_emotions):\n",
    "    for idx, (pred, true) in zip(predicted_emotions.index, zip(predicted_emotions['Emotions'], true_emotions['Emotions'])):\n",
    "        if len(true) >= 2:\n",
    "            true_emotions.at[idx, 'Emotions'] = [emotion for emotion in true if emotion in pred] if any(emotion in pred for emotion in true) else []\n",
    "\n",
    "remove_extra_emotions(predicted_emotions, true_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 感情ラベルを数値に変換する関数を定義\n",
    "def label_to_index(label):\n",
    "    return emotion_names_jp.index(label)\n",
    "\n",
    "# 予測値と正解値の感情ラベルを数値に変換\n",
    "predicted_indices = [label_to_index(label) for labels in predicted_emotions['Emotions'] for label in labels]\n",
    "true_indices = [label_to_index(label) for labels in true_emotions['Emotions'] for label in labels]\n",
    "\n",
    "# 混同行列を作成\n",
    "confusion_matrix_data = confusion_matrix(true_indices, predicted_indices)\n",
    "\n",
    "# 混同行列をDataFrameに変換\n",
    "confusion_matrix_df = pd.DataFrame(confusion_matrix_data, index=[f'真: {label}' for label in emotion_names_jp], columns=[f'予: {label}' for label in emotion_names_jp])\n",
    "\n",
    "# 各行と各列の合計を追加\n",
    "confusion_matrix_df['合計'] = confusion_matrix_df.sum(axis=1)\n",
    "confusion_matrix_df.loc['合計'] = confusion_matrix_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>予: 喜び</th>\n",
       "      <th>予: 悲しみ</th>\n",
       "      <th>予: 期待</th>\n",
       "      <th>予: 驚き</th>\n",
       "      <th>予: 怒り</th>\n",
       "      <th>予: 恐れ</th>\n",
       "      <th>予: 嫌悪</th>\n",
       "      <th>予: 信頼</th>\n",
       "      <th>合計</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>真: 喜び</th>\n",
       "      <td>719</td>\n",
       "      <td>31</td>\n",
       "      <td>82</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>真: 悲しみ</th>\n",
       "      <td>12</td>\n",
       "      <td>734</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>真: 期待</th>\n",
       "      <td>75</td>\n",
       "      <td>28</td>\n",
       "      <td>718</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>真: 驚き</th>\n",
       "      <td>47</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>766</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>真: 怒り</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>真: 恐れ</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>776</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>真: 嫌悪</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>828</td>\n",
       "      <td>0</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>真: 信頼</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>912</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>合計</th>\n",
       "      <td>866</td>\n",
       "      <td>854</td>\n",
       "      <td>876</td>\n",
       "      <td>895</td>\n",
       "      <td>891</td>\n",
       "      <td>916</td>\n",
       "      <td>895</td>\n",
       "      <td>913</td>\n",
       "      <td>7106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        予: 喜び  予: 悲しみ  予: 期待  予: 驚き  予: 怒り  予: 恐れ  予: 嫌悪  予: 信頼    合計\n",
       "真: 喜び     719      31     82     43      1     13      7      1   897\n",
       "真: 悲しみ     12     734     36     36      1     60     23      0   902\n",
       "真: 期待      75      28    718     22      0     25      6      0   874\n",
       "真: 驚き      47      14     16    766      0     31     14      0   888\n",
       "真: 怒り       0       0      0      0    886      0      0      0   886\n",
       "真: 恐れ       7      32     22     20      0    776     17      0   874\n",
       "真: 嫌悪       6      15      2      8      3     11    828      0   873\n",
       "真: 信頼       0       0      0      0      0      0      0    912   912\n",
       "合計        866     854    876    895    891    916    895    913  7106"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各感情ラベルのPrecision、Recall、F1スコアを計算する関数を定義\n",
    "def calculate_metrics(confusion_matrix):\n",
    "    metrics = {}\n",
    "    for i in range(len(emotion_names_jp)):\n",
    "        true_positive = confusion_matrix[i, i]\n",
    "        false_positive = confusion_matrix[:, i].sum() - true_positive\n",
    "        false_negative = confusion_matrix[i, :].sum() - true_positive\n",
    "        precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        metrics[emotion_names_jp[i]] = {'Precision': precision, 'Recall': recall, 'F1 Score': f1_score}\n",
    "    return metrics\n",
    "\n",
    "# 混同行列からPrecision、Recall、F1スコアを計算\n",
    "metrics = calculate_metrics(confusion_matrix_data)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>喜び</th>\n",
       "      <td>0.830254</td>\n",
       "      <td>0.801561</td>\n",
       "      <td>0.815655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>悲しみ</th>\n",
       "      <td>0.859485</td>\n",
       "      <td>0.813747</td>\n",
       "      <td>0.835991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>期待</th>\n",
       "      <td>0.819635</td>\n",
       "      <td>0.821510</td>\n",
       "      <td>0.820571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>驚き</th>\n",
       "      <td>0.855866</td>\n",
       "      <td>0.862613</td>\n",
       "      <td>0.859226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>怒り</th>\n",
       "      <td>0.994388</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>恐れ</th>\n",
       "      <td>0.847162</td>\n",
       "      <td>0.887872</td>\n",
       "      <td>0.867039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>嫌悪</th>\n",
       "      <td>0.925140</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.936652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>信頼</th>\n",
       "      <td>0.998905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Precision    Recall  F1 Score\n",
       "喜び    0.830254  0.801561  0.815655\n",
       "悲しみ   0.859485  0.813747  0.835991\n",
       "期待    0.819635  0.821510  0.820571\n",
       "驚き    0.855866  0.862613  0.859226\n",
       "怒り    0.994388  1.000000  0.997186\n",
       "恐れ    0.847162  0.887872  0.867039\n",
       "嫌悪    0.925140  0.948454  0.936652\n",
       "信頼    0.998905  1.000000  0.999452"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/workspace/0212_omg_its_working_finally.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/0212_omg_its_working_finally\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
